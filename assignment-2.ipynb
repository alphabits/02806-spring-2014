{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Assignment 2 - Anders H\u00f8rsted (s082382)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 1 - Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part we recreate a simple Naive Bayes classifier for spam detection from the book Programming Collective Intelligence <small>(Segaran, 2007)</small> (called PCI from now on). The code for the naive bayes classifier class is found in the file <code>classifiers.py</code> in the same folder as this notebook and only a few parts of the code is included in this notebook. The parts included in the notebook isn't runable but only included to illustrate some details. The various helper functions (<code>get_words</code>, <code>sampletraing</code> etc.) is included directly in this notebook.\n",
      "\n",
      "The first step in creating the classifier is to create a method to extract features from a given email. The funtion is slightly rewritten to follow common Python coding practice and is shown below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def get_words(doc):\n",
      "    splitter = re.compile('\\\\W*')\n",
      "    words = (s.lower() for s in splitter.split(doc) if 2 < len(s) < 20)\n",
      "    return {w: 1 for w in words}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is worth noticing that <code>get_words</code> isn't returning the word count from the document but just a flag indicating whether the word was found in the document or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_words(\"A test with repeating words test\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "{'repeating': 1, 'test': 1, 'with': 1, 'words': 1}"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to code the base class for the classifier. In <small>(Segaran, 2007)</small> the class is called <code>classifier</code> but in this implementation it is called <code>BaseClassifier</code> to follow Python naming standards and better express its purpose. Also all the properties are renamed and the <code>defaultdict</code> class is used for the dictionaries to make the code more readable. The <code>incf</code> function is shown below as an example of the refactoring"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Is part of the BaseClassifier class\n",
      "def increase_feature_category_count(self, feature, category):\n",
      "    self.feature_category_combinations[feature][category] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the various counting functions created we can now create the <code>train</code> function and run through the steps from page 121 in PCI"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from classifiers import BaseClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = BaseClassifier(get_words)\n",
      "cl.train('the quick brown fox jumps over the lazy dog', 'good')\n",
      "cl.train('make quick money in the online casino', 'bad')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.feature_category_count('quick', 'good')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.feature_category_count('quick', 'bad')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For convenience a <code>sampletrain</code> function is created to easily train a classifier instance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "samples = [\n",
      "    ['Nobody owns the water', 'good'],\n",
      "    ['the quick rabbit jump fences', 'good'],\n",
      "    ['buy pharmaceuticals now', 'bad'],\n",
      "    ['make quick money at the online casino', 'bad'],\n",
      "    ['the quick brown fox jumps', 'good']\n",
      "]\n",
      "\n",
      "def sampletrain(classifier):\n",
      "    for doc, category in samples:\n",
      "        self.train(doc, category)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The sampletrain function calls <code>train</code> with 5 different training samples. When calling <code>train</code> the sample is converted to a feature dictionary by the <code>get_words</code> function and the two internal count dictionaries <code>_feature_category_counts</code> and <code>_category_counts</code> are updated in the classifier instance. The two count dictionaries are used by the classifier to calculate estimates of the conditional probability $P(X | C)$ where $X$ is a feature and $C$ is a given class.\n",
      "\n",
      "The conditional probability $P(X | C)$ is the probability of a document containing the feature $X$ <strong>given we know</strong> the document belongs to the class $C$. In other words we find the proportion of documents of class $C$ that contains the feature $X$. In formula \n",
      "\n",
      "$$P(X | C) = \\frac{P(XC)}{P(C)}$$\n",
      "\n",
      "So if eg. the probability of the word 'Quick' given the category 'Good' is 0.5, this means that of all documents of the category 'Good', half of them contains the word 'Quick'. So to estimate $P(X | C)$ in our spam example we count all combinations of classes and features and saves these in the dictionary <code>_feature_category_counts</code>, and also counts the number of documents of each class and saves these in <code>_category_counts</code>. There is a problem by using this estimation method. Since the feature space is huge many features will only be present in a few training documents making the estimator very sensitive when training on small training sets. Also if a feature isn't included in the training data, the conditional probability will be 0. To avoid these problems an assumed probability is calculated. To calculate it we define a weight $w$ and an initial probability $p_0$ that is the probability of the feature if not included in the training data. If $n_x$ is the feature count across all classes and $p$ is the estimate of $P(X | C)$ the assumed probability $p_a$ becomes\n",
      "\n",
      "$$p_a = \\frac{wp_0 + n_xp}{w + n_x}$$\n",
      "\n",
      "Which is just a weighted average of the initial probability $p_0$ and the conditional probability $p$, weighted by the weight $w$ and the feature count $n_x$\n",
      "\n",
      "So far we have only talked of the conditional probability of a single feature for a given class $P(X | C)$. The probability of interest is the conditional probability of a class given a feature vector $P(C | \\mathbf{X})$. \n",
      "\n",
      "The first problem is to combine $P(C | X)$ to get $P(C | \\mathbf{X})$ and secondly to \"reverse\" $P(\\mathbf{X} | C)$ to get $P(C | \\mathbf{X})$.\n",
      "The combination of $P(X|C)$ to get $P(\\mathbf{X}|C)$ is what gives Naive Bayes its name. Without further justification is is simply assumed that the conditional probability of each feature $P(X_i | C)$ is independent. The probability of a document is therefore just the product of its features $X_i$.\n",
      "\n",
      "$$P(\\mathbf{X} | C) = \\prod_i{P(X_i|C)}$$\n",
      "\n",
      "Secondly to \"reverse\" $P(\\mathbf{X} | C)$ Bayes' Theorem is used. Bayes Theorem is just a rearangement of the multiplication rule combined with the symmetry of the multiplication rule. So starting with\n",
      "\n",
      "$$P(C\\mathbf{X}) = P(C)P(\\mathbf{X}|C)$$\n",
      "\n",
      "and\n",
      "\n",
      "$$P(C\\mathbf{X}) = P(\\mathbf{X}C) = P(\\mathbf{X})P(C|\\mathbf{X})$$\n",
      "\n",
      "gives\n",
      "\n",
      "$$P(C|\\mathbf{X}) = \\frac{P(C\\mathbf{X})}{P(\\mathbf{X})} = \\frac{P(C)P(\\mathbf{X}|C)}{P(\\mathbf{X})}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from classifiers import NaiveBayesClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = NaiveBayesClassifier(get_words)\n",
      "sampletrain(cl)\n",
      "cl.probability_of_feature_given_category('quick', 'good')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.6666666666666666"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.evidence_of_category_given_document('quick rabbit', 'good')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.15624999999999997"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.evidence_of_category_given_document('quick rabbit', 'bad')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.05"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.probability_of_category('good')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.6"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = NaiveBayesClassifier(get_words)\n",
      "sampletrain(cl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.classify('quick rabbit')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "'good'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.classify('quick money')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "'bad'"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.set_threshold_for_category('bad', 3.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl.classify('quick money')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "'UNKNOWN'"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for _ in range(10): \n",
      "    sampletrain(cl)\n",
      "cl.classify('quick money')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "'bad'"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 2 - Classify tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import ifilter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('data/englishtweets.txt', 'r') as f:\n",
      "    tweets = f.readlines()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def has_happy_face(tweet):\n",
      "    return ':)' in tweet or ':-)' in tweet\n",
      "\n",
      "def has_sad_face(tweet):\n",
      "    return ':(' in tweet or ':-(' in tweet\n",
      "\n",
      "def tweet_mood_is_defined(tweet):\n",
      "    return has_happy_face(tweet) ^ has_sad_face(tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets_with_mood = ifilter(tweet_mood_is_defined, tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twitter_username_pattern = r'(?<=\\W)@\\w{1,15}(?=\\W)'\n",
      "def clean_tweet(tweet):\n",
      "    return re.sub(twitter_username_pattern, \" \", \" \"+tweet+\" \", flags=re.I).strip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_tweets = map(clean_tweet, tweets_with_mood)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split_index = len(clean_tweets) / 2\n",
      "training_data, test_data = clean_tweets[:split_index], clean_tweets[split_index:]\n",
      "\n",
      "test_data_happy = filter(has_happy_face, test_data)\n",
      "test_data_sad = filter(has_sad_face, test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cl = NaiveBayesClassifier(get_words)\n",
      "for tweet in training_data:\n",
      "    category = 'happy' if has_happy_face(tweet) else 'sad'\n",
      "    cl.train(tweet, category)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_data_happy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "641677"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_data_sad)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 194,
       "text": [
        "219745"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_happy = [cl.classify(t) for t in test_data_happy]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_sad = [cl.classify(t) for t in test_data_sad]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len([c for c in result_sad if c != 'happy'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "76815"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "34./607"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 191,
       "text": [
        "0.05601317957166392"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seagaran, Toby. (2007) - <em>Programming Collective Intelligence</em> - O'Reilly Media"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}